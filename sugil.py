import streamlit as st
import google.generativeai as genai
from PIL import Image

# --- 1. ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ìˆ˜ê¸¸ì´ - ìˆ˜í•™ì˜ ê¸¸ì¡ì´", page_icon="ğŸ“")
st.title("ğŸ§‘â€ğŸ« ìˆ˜ê¸¸ì´: ìˆ˜í•™ì˜ ê¸¸ì¡ì´")

# --- 2. API í‚¤ ì„¤ì • ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google AI Studio API Key", type="password")

# --- 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìˆ˜ê¸¸ì´ í˜ë¥´ì†Œë‚˜) ---
# GemmaëŠ” ì‹œìŠ¤í…œ ì„¤ì • ì¹¸ì´ ì—†ìœ¼ë¯€ë¡œ, ì§ˆë¬¸ ì•ì— ë¶™ì¼ í…ìŠ¤íŠ¸ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.
system_prompt_text = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì‹¤ë ¥ ìˆëŠ” ìˆ˜í•™ íŠœí„° 'ìˆ˜ê¸¸ì´'ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ì¹™ì„ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”:
1. LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: $x^2 + 2x$).
2. í’€ì´ëŠ” ë‹¨ê³„ë³„(Step-by-step)ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3. ì„¤ëª… ëì—ëŠ” "ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤."ë¼ë©° ì˜ˆì œë¥¼ í•˜ë‚˜ ì œì‹œí•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
"""

# --- 4. ì„¸ì…˜ ìƒíƒœ (ëŒ€í™” ê¸°ë¡) ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# --- 5. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

uploaded_file = st.sidebar.file_uploader("ë¬¸ì œ ì‚¬ì§„ ì—…ë¡œë“œ", type=["jpg", "png", "jpeg", "webp"])

# --- 6. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ì‚¬ì§„ì„ ì˜¬ë¦¬ê³  'í’€ì–´ì¤˜'ë¼ê³  í•˜ì„¸ìš”."):
    if not api_key:
        st.error("API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤! ì‚¬ì´ë“œë°”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    genai.configure(api_key=api_key)

    # UIì— ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    image_input = None
    if uploaded_file:
        image_input = Image.open(uploaded_file)
        with st.chat_message("user"):
            st.image(image_input, width=200)
            
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- [í•µì‹¬] Gemma 3 ë‹¨ì¼ ëª¨ë¸ ì„¤ì • ---
    # ì´ì œ ë³µì¡í•œ ë¶„ê¸° ì²˜ë¦¬ ì—†ì´ Gemma í•˜ë‚˜ë¡œ í†µì¼í•©ë‹ˆë‹¤!
    model = genai.GenerativeModel(model_name="gemma-3-27b-it")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í˜ë¥´ì†Œë‚˜ ì£¼ì… + ì§ˆë¬¸ + ì´ë¯¸ì§€)
    combined_text = system_prompt_text + "\n\nì‚¬ìš©ì ì§ˆë¬¸: " + prompt
    
    if image_input:
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ì„œ ì „ë‹¬
        final_prompt = [combined_text, image_input]
    else:
        # í…ìŠ¤íŠ¸ë§Œ ìˆìœ¼ë©´ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì „ë‹¬
        final_prompt = combined_text

    # --- 7. AI ì‘ë‹µ ìƒì„± ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # Gemmaì—ê²Œ ì§ˆë¬¸ ì „ì†¡ (ì´ë¯¸ì§€ í¬í•¨ ê°€ëŠ¥!)
            response = model.generate_content(final_prompt, stream=True)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (ì˜¤ë¥˜ ë°©ì§€ ë¡œì§ í¬í•¨)
            for chunk in response:
                try:
                    if chunk.text:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")
                except Exception:
                    pass # ë§ˆì§€ë§‰ ë¹ˆ ì¡°ê°ì€ ë¬´ì‹œ
            
            message_placeholder.markdown(full_response)
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
