import streamlit as st
import google.generativeai as genai
from PIL import Image

# --- 1. ê¸°ë³¸ ì„¤ì • ë° ë””ìì¸ ---
st.set_page_config(page_title="ìˆ˜ê¸¸ì´ - ìˆ˜í•™ì˜ ê¸¸ì¡ì´", page_icon="ğŸ“")
st.title("ğŸ§‘â€ğŸ« ìˆ˜ê¸¸ì´: ìˆ˜í•™ì˜ ê¸¸ì¡ì´")

# --- [ìˆ˜ì •ëœ ë¶€ë¶„] API í‚¤ ì²˜ë¦¬ ë¡œì§ ---
# secrets.toml íŒŒì¼ì— í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ , ì—†ìœ¼ë©´ ì‚¬ì´ë“œë°” ì…ë ¥ì°½ì„ ë„ì›ë‹ˆë‹¤.
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google AI Studio API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

# --- 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìˆ˜ê¸¸ì´ì˜ í˜ë¥´ì†Œë‚˜) ---
system_prompt = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì‹¤ë ¥ ìˆëŠ” ìˆ˜í•™ íŠœí„° 'ìˆ˜ê¸¸ì´'ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ì¹™ì„ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”:
1. LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: $x^2 + 2x$).
2. í’€ì´ëŠ” ë‹¨ê³„ë³„(Step-by-step)ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3. ì„¤ëª…ì´ ëë‚œ í›„ì—ëŠ” ë°˜ë“œì‹œ "ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. í•œë²ˆ í’€ì–´ë³´ì‹œê² ì–´ìš”?"ë¼ê³  ë§í•˜ê³ , ë¹„ìŠ·í•œ ë‚œì´ë„ì˜ ì˜ˆì œë¥¼ í•˜ë‚˜ ì œì‹œí•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
"""

# --- 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# --- 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ---
# ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì´ë¯¸ì§€ ì—…ë¡œë”
uploaded_file = st.sidebar.file_uploader("ë¬¸ì œ ì‚¬ì§„ ì—…ë¡œë“œ", type=["jpg", "png", "jpeg", "webp"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ì‚¬ì§„ì„ ì˜¬ë¦¬ê³  'í’€ì–´ì¤˜'ë¼ê³  í•˜ì„¸ìš”."):
    if not api_key:
        st.error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.tomlì„ í™•ì¸í•˜ê±°ë‚˜ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # Gemini ì„¤ì •
    genai.configure(api_key=api_key)
    
    # ëª¨ë¸ ì„¤ì • (ìš”ì²­í•˜ì‹  gemini-2.5-flash-lite ìœ ì§€)
    model = genai.GenerativeModel(
        model_name="gemma-3-27b",
        system_instruction=system_prompt
    )

    # ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image_input = None
    if uploaded_file:
        image_input = Image.open(uploaded_file)
        with st.chat_message("user"):
            st.image(image_input, caption="ì—…ë¡œë“œí•œ ë¬¸ì œ", use_column_width=False, width=200)

    # ëŒ€í™” ê¸°ë¡ ì €ì¥ (UIìš©)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- 5. Gemini API í˜¸ì¶œ ì¤€ë¹„ ---
    chat_history = []
    for msg in st.session_state.messages[:-1]:
        role = "user" if msg["role"] == "user" else "model"
        chat_history.append({"role": role, "parts": [msg["content"]]})

    current_parts = [prompt]
    if image_input:
        current_parts.append(image_input)

    # --- 6. AI ì‘ë‹µ ìƒì„± (Stream) ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            chat = model.start_chat(history=chat_history)
            response = chat.send_message(current_parts, stream=True)
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
            # AI ì‘ë‹µ ê¸°ë¡ ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


