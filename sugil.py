import streamlit as st
import google.generativeai as genai
from PIL import Image

# --- 1. ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ìˆ˜ê¸¸ì´ (Gemma ì‹¤í—˜ì‹¤)", page_icon="ğŸ§ª")
st.title("ğŸ§ª ìˆ˜ê¸¸ì´: Gemma 3 ì‹œë ¥ í…ŒìŠ¤íŠ¸ ì¤‘")
st.warning("âš ï¸ ì´ ë²„ì „ì€ ì‹¤í—˜ìš©ì…ë‹ˆë‹¤! Gemmaê°€ ì‚¬ì§„ì„ ë³¼ ìˆ˜ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")

# --- 2. API í‚¤ ì„¤ì • ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google AI Studio API Key", type="password")

# --- 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê¼¼ìˆ˜ìš©) ---
system_prompt_text = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì‹¤ë ¥ ìˆëŠ” ìˆ˜í•™ íŠœí„° 'ìˆ˜ê¸¸ì´'ì…ë‹ˆë‹¤.
LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , ë§ˆì§€ë§‰ì—” ìœ ì‚¬ ë¬¸ì œë¥¼ ì œì•ˆí•˜ì„¸ìš”.
"""

# --- 4. ì„¸ì…˜ ìƒíƒœ ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# --- 5. ì±„íŒ… í™”ë©´ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

uploaded_file = st.sidebar.file_uploader("ë¬¸ì œ ì‚¬ì§„ ì—…ë¡œë“œ", type=["jpg", "png", "jpeg", "webp"])

# --- 6. ì…ë ¥ ì²˜ë¦¬ ---
if prompt := st.chat_input("ì‚¬ì§„ì„ ì˜¬ë¦¬ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
    if not api_key:
        st.error("API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        st.stop()

    genai.configure(api_key=api_key)

    # UI í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    image_input = None
    if uploaded_file:
        image_input = Image.open(uploaded_file)
        with st.chat_message("user"):
            st.image(image_input, width=200, caption="Gemmaì—ê²Œ ì´ ì‚¬ì§„ì„ ë³´ëƒ…ë‹ˆë‹¤...")
            
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- [ì‹¤í—˜ í•µì‹¬] ë¬´ì¡°ê±´ Gemmaë§Œ ì‚¬ìš©! ---
    selected_model = "gemma-3-27b-it"  # íƒ€í˜‘ì€ ì—†ë‹¤. ë¬´ì¡°ê±´ Gemma!
    
    # GemmaëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì§€ì› ì•ˆ í•˜ë¯€ë¡œ ë¹„ì›Œì„œ ìƒì„±
    model = genai.GenerativeModel(model_name=selected_model)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ ì„¤ì • + ì§ˆë¬¸ + ì´ë¯¸ì§€)
    if uploaded_file:
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì„œ ë³´ëƒ„ (ë©€í‹°ëª¨ë‹¬ ì‹œë„)
        # ê¼¼ìˆ˜: í…ìŠ¤íŠ¸ ë¶€ë¶„ì— í˜ë¥´ì†Œë‚˜ë¥¼ ì„ì–´ì„œ ë³´ëƒ„
        combined_text = system_prompt_text + "\n\nì‚¬ìš©ì ì§ˆë¬¸: " + prompt
        final_prompt = [combined_text, image_input] 
    else:
        # í…ìŠ¤íŠ¸ë§Œ ìˆì„ ë•Œ
        final_prompt = system_prompt_text + "\n\nì‚¬ìš©ì ì§ˆë¬¸: " + prompt

    # --- 7. ì‘ë‹µ ìƒì„± ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìš”ì²­
            response = model.generate_content(final_prompt, stream=True)
            
            for chunk in response:
                try:
                    if chunk.text:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")
                except Exception:
                    pass
            
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
            # ì„±ê³µí–ˆë‹¤ë©´ ì¶•í•˜ ë©”ì‹œì§€!
            if uploaded_file and full_response:
                st.balloons()
                st.success("ğŸ‰ ëŒ€ë°•! Gemma 3ê°€ ì´ë¯¸ì§€ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤! ì´ì œ ì™„ì „ ë¬´ì œí•œì…ë‹ˆë‹¤!")

        except Exception as e:
            # ì‹¤íŒ¨í•˜ë©´ ì›ì¸ ë¶„ì„ ë©”ì‹œì§€ ì¶œë ¥
            st.error("ğŸ§ª ì‹¤í—˜ ì‹¤íŒ¨!")
            if "400" in str(e) or "Media not supported" in str(e) or "multimodal" in str(e):
                st.error(f"ê²°ë¡ : '{selected_model}' ëª¨ë¸ì€ ì—­ì‹œ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ì—†ë„¤ìš”. (í…ìŠ¤íŠ¸ ì „ìš©)")
                st.info("ğŸ‘‰ ë‹¤ì‹œ ì´ì „ì˜ 'í•˜ì´ë¸Œë¦¬ë“œ ì½”ë“œ(Gemma+Gemini)'ë¡œ ë³µêµ¬í•´ì£¼ì„¸ìš”.")
            else:
                st.error(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
