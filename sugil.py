import streamlit as st
import google.generativeai as genai
from PIL import Image

# --- 1. ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ìˆ˜ê¸¸ì´ - ìˆ˜í•™ì˜ ê¸¸ì¡ì´", page_icon="ğŸ“")
st.title("ğŸ§‘â€ğŸ« ìˆ˜ê¸¸ì´: ìˆ˜í•™ì˜ ê¸¸ì¡ì´")

# --- 2. API í‚¤ ì„¤ì • ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Google AI Studio API Key", type="password")

# --- 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìˆ˜ê¸¸ì´ì˜ í˜ë¥´ì†Œë‚˜) ---
# ì´ ë‚´ìš©ì€ ë³€ìˆ˜ì—ë§Œ ë‹´ì•„ë‘ê³ , ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì£¼ì…í•©ë‹ˆë‹¤.
system_prompt_text = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì‹¤ë ¥ ìˆëŠ” ìˆ˜í•™ íŠœí„° 'ìˆ˜ê¸¸ì´'ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ì¹™ì„ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”:
1. LaTeX ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: $x^2 + 2x$).
2. í’€ì´ëŠ” ë‹¨ê³„ë³„(Step-by-step)ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3. ì„¤ëª… ëì—ëŠ” "ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤."ë¼ë©° ì˜ˆì œë¥¼ í•˜ë‚˜ ì œì‹œí•˜ì„¸ìš”.
"""

# --- 4. ì„¸ì…˜ ìƒíƒœ ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# --- 5. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

uploaded_file = st.sidebar.file_uploader("ë¬¸ì œ ì‚¬ì§„ ì—…ë¡œë“œ", type=["jpg", "png", "jpeg", "webp"])

# --- 6. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ì‚¬ì§„ì„ ì˜¬ë¦¬ê³  'í’€ì–´ì¤˜'ë¼ê³  í•˜ì„¸ìš”."):
    if not api_key:
        st.error("API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        st.stop()

    genai.configure(api_key=api_key)

    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    image_input = None
    if uploaded_file:
        image_input = Image.open(uploaded_file)
        with st.chat_message("user"):
            st.image(image_input, width=200)
            
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- [í•µì‹¬] ëª¨ë¸ ì„ íƒ ë° ì„¤ì • ë¶„ê¸° ---
    if uploaded_file:
        # [ìƒí™© A] ì‚¬ì§„ì´ ìˆìŒ -> Gemini (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì§€ì› O)
        selected_model = "gemini-2.5-flash-lite"
        ai_name = "Gemini"
        
        # GeminiëŠ” ì •ì„ëŒ€ë¡œ ì„¤ì •
        model = genai.GenerativeModel(
            model_name=selected_model,
            system_instruction=system_prompt_text 
        )
        final_prompt = [prompt, image_input] # ì´ë¯¸ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ì„œ
        
    else:
        # [ìƒí™© B] ì‚¬ì§„ ì—†ìŒ -> Gemma (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì§€ì› X)
        selected_model = "gemma-3-27b-it"
        ai_name = "Gemma"
        
        # GemmaëŠ” system_instruction íŒŒë¼ë¯¸í„°ë¥¼ ì•„ì˜ˆ ë¹¼ì•¼ í•©ë‹ˆë‹¤! (ì´ê²Œ ì˜¤ë¥˜ ì›ì¸)
        model = genai.GenerativeModel(
            model_name=selected_model
        )
        
        # ëŒ€ì‹  ì§ˆë¬¸ ì•ì— í˜ë¥´ì†Œë‚˜ë¥¼ 'ëª°ë˜' ë¶™ì—¬ì„œ ë³´ëƒ…ë‹ˆë‹¤.
        final_prompt = system_prompt_text + "\n\nì‚¬ìš©ì ì§ˆë¬¸: " + prompt

    # --- 7. AI ì‘ë‹µ ìƒì„± ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # st.caption(f"ğŸš€ {ai_name} ëª¨ë¸ì´ ë‹µë³€ ì¤‘... (ë¬´ë£Œ)") # ë””ë²„ê¹…ìš©

            # íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ (Gemmaì˜ ê²½ìš° íˆìŠ¤í† ë¦¬ ê´€ë¦¬ê°€ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆì–´ 1íšŒì„± ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë‹¨ìˆœí™”)
            # ì—¬ê¸°ì„œëŠ” ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ 'ì±„íŒ… ê¸°ë¡' ê¸°ëŠ¥ì€ Geminiì¼ ë•Œë§Œ ì™„ë²½ ì§€ì›í•˜ê³ 
            # GemmaëŠ” í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ê²Œ í•©ë‹ˆë‹¤. (ê°€ì¥ ì•ˆì „í•œ ë°©ë²•)
            
            if selected_model == "gemma-3-27b-it":
                # GemmaëŠ” ì±„íŒ… ê¸°ë¡ ì—†ì´ ë°”ë¡œ ìƒì„± (ì˜¤ë¥˜ ìµœì†Œí™”)
                response = model.generate_content(final_prompt, stream=True)
            else:
                # GeminiëŠ” ì±„íŒ… ê¸°ë¡ í¬í•¨
                chat_history = []
                for msg in st.session_state.messages[:-1]:
                    role = "user" if msg["role"] == "user" else "model"
                    chat_history.append({"role": role, "parts": [msg["content"]]})
                
                chat = model.start_chat(history=chat_history)
                response = chat.send_message(final_prompt, stream=True)
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            if "400" in str(e):
                st.error(f"ì„¤ì • ì˜¤ë¥˜: {e}")
            elif "404" in str(e):
                st.error(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {selected_model}")
            elif "429" in str(e):
                st.error("ì‚¬ìš©ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
